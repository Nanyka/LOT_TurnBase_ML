{
    "name": "root",
    "gauges": {
        "GroupJumper.Policy.Entropy.mean": {
            "value": 1.5207040309906006,
            "min": 1.3886165618896484,
            "max": 2.5188848972320557,
            "count": 31
        },
        "GroupJumper.Policy.Entropy.sum": {
            "value": 76643.484375,
            "min": 69986.2734375,
            "max": 126951.8046875,
            "count": 31
        },
        "GroupJumper.Environment.EpisodeLength.mean": {
            "value": 49.0,
            "min": 49.0,
            "max": 49.0,
            "count": 31
        },
        "GroupJumper.Environment.EpisodeLength.sum": {
            "value": 49392.0,
            "min": 47628.0,
            "max": 49392.0,
            "count": 31
        },
        "GroupJumper.Step.mean": {
            "value": 1549950.0,
            "min": 49950.0,
            "max": 1549950.0,
            "count": 31
        },
        "GroupJumper.Step.sum": {
            "value": 1549950.0,
            "min": 49950.0,
            "max": 1549950.0,
            "count": 31
        },
        "GroupJumper.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 16.625808715820312,
            "min": -2.1791012287139893,
            "max": 16.625808715820312,
            "count": 31
        },
        "GroupJumper.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 16625.80859375,
            "min": -2176.922119140625,
            "max": 16625.80859375,
            "count": 31
        },
        "GroupJumper.Policy.ExtrinsicValueEstimate.mean": {
            "value": 16.595582962036133,
            "min": -2.12434458732605,
            "max": 16.595582962036133,
            "count": 31
        },
        "GroupJumper.Policy.ExtrinsicValueEstimate.sum": {
            "value": 16595.58203125,
            "min": -2122.22021484375,
            "max": 16595.58203125,
            "count": 31
        },
        "GroupJumper.Environment.CumulativeReward.mean": {
            "value": -0.09855999685451389,
            "min": -0.13720999467931688,
            "max": -0.06994999754987657,
            "count": 31
        },
        "GroupJumper.Environment.CumulativeReward.sum": {
            "value": -98.55999685451388,
            "min": -137.20999467931688,
            "max": -69.94999754987657,
            "count": 31
        },
        "GroupJumper.Policy.ExtrinsicReward.mean": {
            "value": 8.188250360913575,
            "min": -1.3348345976692062,
            "max": 8.188250360913575,
            "count": 31
        },
        "GroupJumper.Policy.ExtrinsicReward.sum": {
            "value": 8188.250360913575,
            "min": -1333.499763071537,
            "max": 8188.250360913575,
            "count": 31
        },
        "GroupJumper.Environment.GroupCumulativeReward.mean": {
            "value": 8.4840004722327,
            "min": -1.0172970714690808,
            "max": 8.4840004722327,
            "count": 31
        },
        "GroupJumper.Environment.GroupCumulativeReward.sum": {
            "value": 8484.0004722327,
            "min": -1016.2797743976116,
            "max": 8484.0004722327,
            "count": 31
        },
        "GroupJumper.Losses.PolicyLoss.mean": {
            "value": 0.19803723697833714,
            "min": 0.19673909145228893,
            "max": 0.20256901433715202,
            "count": 31
        },
        "GroupJumper.Losses.PolicyLoss.sum": {
            "value": 5.54504263539344,
            "min": 5.315555276671407,
            "max": 5.657590861699039,
            "count": 31
        },
        "GroupJumper.Losses.ValueLoss.mean": {
            "value": 6.920857940575577,
            "min": 0.7704529118439623,
            "max": 20.620725627031913,
            "count": 31
        },
        "GroupJumper.Losses.ValueLoss.sum": {
            "value": 193.78402233611615,
            "min": 20.80222861978698,
            "max": 577.3803175568936,
            "count": 31
        },
        "GroupJumper.Losses.BaselineLoss.mean": {
            "value": 14.038268290143334,
            "min": 0.9168558065186703,
            "max": 33.66247771146904,
            "count": 31
        },
        "GroupJumper.Losses.BaselineLoss.sum": {
            "value": 393.07151212401334,
            "min": 24.755106776004098,
            "max": 942.549375921133,
            "count": 31
        },
        "GroupJumper.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.0003,
            "count": 31
        },
        "GroupJumper.Policy.LearningRate.sum": {
            "value": 0.0084,
            "min": 0.0081,
            "max": 0.0084,
            "count": 31
        },
        "GroupJumper.Policy.Epsilon.mean": {
            "value": 0.19999999999999998,
            "min": 0.19999999999999998,
            "max": 0.19999999999999998,
            "count": 31
        },
        "GroupJumper.Policy.Epsilon.sum": {
            "value": 5.6,
            "min": 5.3999999999999995,
            "max": 5.6,
            "count": 31
        },
        "GroupJumper.Policy.Beta.mean": {
            "value": 0.009999999999999998,
            "min": 0.009999999999999998,
            "max": 0.009999999999999998,
            "count": 31
        },
        "GroupJumper.Policy.Beta.sum": {
            "value": 0.27999999999999997,
            "min": 0.26999999999999996,
            "max": 0.27999999999999997,
            "count": 31
        },
        "GroupJumper.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 31
        },
        "GroupJumper.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 31
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1680158254",
        "python_version": "3.9.13 (main, Oct 13 2022, 16:12:30) \n[Clang 12.0.0 ]",
        "command_line_arguments": "/Users/admin/opt/anaconda3/envs/my_rl_env/bin/mlagents-learn /Users/admin/Desktop/GameProjects/TestMLAgents/config/groupjumper_skill2_v4.yaml --env=/Users/admin/Desktop/GroupAgent_skill2_v4.app --run-id=GroupJumper_Skill2_v4 --no-graphic",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.11.0",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1680164770"
    },
    "total": 6516.454438006,
    "count": 1,
    "self": 0.377185747999647,
    "children": {
        "run_training.setup": {
            "total": 0.03343720399999994,
            "count": 1,
            "self": 0.03343720399999994
        },
        "TrainerController.start_learning": {
            "total": 6516.043815054,
            "count": 1,
            "self": 0.7666414709865421,
            "children": {
                "TrainerController._reset_env": {
                    "total": 3.3100999039999994,
                    "count": 1,
                    "self": 3.3100999039999994
                },
                "TrainerController.advance": {
                    "total": 6511.8326595040135,
                    "count": 43751,
                    "self": 0.7789376190585244,
                    "children": {
                        "env_step": {
                            "total": 322.1584972679869,
                            "count": 43751,
                            "self": 269.8989597870507,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 51.74761544396824,
                                    "count": 43751,
                                    "self": 2.5744771739733636,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 49.17313826999488,
                                            "count": 43751,
                                            "self": 49.17313826999488
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.5119220369679329,
                                    "count": 43751,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 6506.724213316985,
                                            "count": 43751,
                                            "is_parallel": true,
                                            "self": 6324.521814324962,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.00068851400000014,
                                                    "count": 1,
                                                    "is_parallel": true,
                                                    "self": 0.00032028400000072565,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0003682299999994143,
                                                            "count": 2,
                                                            "is_parallel": true,
                                                            "self": 0.0003682299999994143
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 182.20171047802285,
                                                    "count": 43751,
                                                    "is_parallel": true,
                                                    "self": 10.34815022306799,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 17.105200186933356,
                                                            "count": 43751,
                                                            "is_parallel": true,
                                                            "self": 17.105200186933356
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 133.63527052198813,
                                                            "count": 43751,
                                                            "is_parallel": true,
                                                            "self": 133.63527052198813
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 21.113089546033358,
                                                            "count": 43751,
                                                            "is_parallel": true,
                                                            "self": 8.866108269050267,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 12.24698127698309,
                                                                    "count": 87502,
                                                                    "is_parallel": true,
                                                                    "self": 12.24698127698309
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 6188.895224616968,
                            "count": 43751,
                            "self": 1.0938715949632751,
                            "children": {
                                "process_trajectory": {
                                    "total": 322.38250475400395,
                                    "count": 43751,
                                    "self": 321.966527908004,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.41597684599992135,
                                            "count": 3,
                                            "self": 0.41597684599992135
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 5865.418848268001,
                                    "count": 875,
                                    "self": 233.1538042297634,
                                    "children": {
                                        "TorchPOCAOptimizer.update": {
                                            "total": 5632.265044038238,
                                            "count": 293892,
                                            "self": 5632.265044038238
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 9.820005288929678e-07,
                    "count": 1,
                    "self": 9.820005288929678e-07
                },
                "TrainerController._save_models": {
                    "total": 0.13441319300000032,
                    "count": 1,
                    "self": 0.0015191709999271552,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.13289402200007316,
                            "count": 1,
                            "self": 0.13289402200007316
                        }
                    }
                }
            }
        }
    }
}